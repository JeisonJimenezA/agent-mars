# PYTHON MODULE IMPLEMENTATION

## CRITICAL RULES (READ FIRST - VIOLATIONS CAUSE CRASHES)

### 1. Dataclass Mutable Defaults (MOST COMMON ERROR)
```python
# WRONG - WILL CRASH:
@dataclass
class Config:
    layers: List[int] = []           # CRASHES!
    params: Dict = {{}}               # CRASHES!

# CORRECT:
from dataclasses import dataclass, field
@dataclass
class Config:
    layers: List[int] = field(default_factory=list)
    params: Dict = field(default_factory=dict)
```

### 2. Encoding & Characters
- **NO EMOJIS**: Use ASCII only. Windows crashes on emoji characters.
- **FILE ENCODING**: Always use `encoding='utf-8'` when opening files.
- **NO PARQUET**: Use `.to_csv()` not `.to_parquet()` (PyArrow not installed).

### 3. Data Types
- **y_train**: Must be 1D integer array (0, 1, 2...), NOT one-hot, NOT probabilities.
- **Statistics**: Never compute class_weights with placeholder data. Load real data first.

### 4. API Compatibility
- **LightGBM**: Use `callbacks=[lgb.early_stopping(100)]` not `early_stopping_rounds=`
- **XGBoost**: Use 1D integer labels for multiclass, not one-hot.

### 5. DATA LEAKAGE PREVENTION (MANDATORY - causes invalid metrics if violated)

**Rule: Fit on train ONLY. Transform val/test separately.**
```python
# WRONG - LEAKAGE (fits scaler on all data, test statistics contaminate train):
scaler = StandardScaler().fit(X_all)
X_train_s = scaler.transform(X_train)

# CORRECT:
scaler = StandardScaler().fit(X_train)   # fit on train ONLY
X_train_s = scaler.transform(X_train)
X_val_s   = scaler.transform(X_val)
X_test_s  = scaler.transform(X_test)
```

The same rule applies to ALL preprocessing objects:
- `LabelEncoder`, `OrdinalEncoder`, `OneHotEncoder` -> fit on train, transform val/test
- `SimpleImputer`, `KNNImputer` -> fit on train, transform val/test
- `PCA`, `SelectKBest`, `TruncatedSVD` -> fit on train, transform val/test
- Target encoding / group statistics -> compute from train fold only

**Rule: No global statistics from the full dataset.**
```python
# WRONG - LEAKAGE (global mean includes validation/test rows):
df['age_norm'] = (df['age'] - df['age'].mean()) / df['age'].std()

# CORRECT:
train_mean = X_train['age'].mean()
train_std  = X_train['age'].std()
X_train['age_norm'] = (X_train['age'] - train_mean) / train_std
X_val['age_norm']   = (X_val['age']   - train_mean) / train_std
X_test['age_norm']  = (X_test['age']  - train_mean) / train_std
```

**Rule: Cross-validation must apply preprocessing INSIDE each fold.**
```python
# WRONG - LEAKAGE (scaler fit on all folds before CV):
X_scaled = scaler.fit_transform(X)
for train_idx, val_idx in kf.split(X_scaled): ...

# CORRECT:
for train_idx, val_idx in kf.split(X):
    scaler = StandardScaler().fit(X[train_idx])
    X_tr = scaler.transform(X[train_idx])
    X_vl = scaler.transform(X[val_idx])
```

**Rule: Never derive features from the target variable using full-dataset information.**
- Group means/counts by target must be computed from train rows only.
- Do NOT include columns that directly encode or trivially predict the target.

---

## OUTPUT FORMAT

Provide COMPLETE working code in a single ```python block:

```python
# {file_name}
# Your implementation here
```

**SIZE LIMIT**: Keep under 400 lines. Prioritize working code over comprehensive code.

---

## YOUR TASK

Implement module: `{file_name}`

**Description**: {file_description}

---

## CONTEXT

### Problem Description
{problem_description}

### EDA Report
{eda_report}

### Solution Approach
{idea}

### Data Locations
- Training: `./train.csv`
- Test: `./test.csv`
- Sample submission: `./sample_submission.csv` or `./input/sample_submission.csv`
- Cache directory: `./working/cache/`

---

## EXISTING MODULES (import from these, do not reimplement)

{library_files}

---

## IMPLEMENTATION REQUIREMENTS

1. **Imports**: Import from existing modules above, don't reimplement.
2. **No main block**: Do NOT include `if __name__ == "__main__":`
3. **Hyperparameters**: Include arguments for dataset size and training steps.
4. **Metrics**: Print full precision without rounding.

### If this module loads data:
- Use `load_cached_data: bool` parameter
- Cache to `./working/cache/` using CSV or NPY (NO pickle, NO parquet)
- Create cache dir: `os.makedirs('./working/cache/', exist_ok=True)`

### If this module trains models:
- Print training/validation metrics during training
- Implement Early Stopping

### If this module generates submissions:
- Save to `./submission/submission.csv`
- Match format from sample submission file

---

Now implement the complete `{file_name}` module:
