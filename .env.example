# .env.example
# Copy this file to .env and fill in your values

# ============================================================================
# PROVIDER SELECTION
# ============================================================================
# Option 1: Use LLM_PROVIDER (recommended)
# Values: ollama, deepseek, openai, anthropic, gemini
LLM_PROVIDER=ollama

# Option 2: Legacy flags (USE_* variables)
# USE_OLLAMA=false
# USE_GEMINI=false
# USE_ANTHROPIC=false
# USE_OPENAI=false
# If all are false, uses DeepSeek as default

# ============================================================================
# OLLAMA Configuration (LOCAL - recommended for privacy/cost)
# ============================================================================
# First install Ollama: https://ollama.com/download
# Then pull a model: ollama pull qwen2.5-coder:14b
OLLAMA_BASE_URL=http://localhost:11434
# Models: qwen2.5-coder:14b (fits 16GB VRAM), qwen2.5-coder:7b (fits 8GB VRAM)
# For 32B with offloading: qwen2.5-coder:32b (needs 16GB+ VRAM + RAM offload)
OLLAMA_MODEL=qwen2.5-coder:14b
# Local models can generate more tokens without cost concerns
OLLAMA_MAX_TOKENS=16384

# ============================================================================
# DEEPSEEK Configuration (cloud, most cost-effective)
# ============================================================================
DEEPSEEK_API_KEY=your-deepseek-api-key-here
DEEPSEEK_BASE_URL=https://api.deepseek.com
# Models: deepseek-chat, deepseek-reasoner
DEEPSEEK_MODEL=deepseek-chat
# DeepSeek V3: max 8192, DeepSeek-Reasoner: max 16384
DEEPSEEK_MAX_TOKENS=8192

# ============================================================================
# OPENAI Configuration
# ============================================================================
OPENAI_API_KEY=your-openai-api-key-here
# Models: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
OPENAI_MODEL=gpt-4o
# GPT-4o: 16K output, GPT-4-turbo: 4K output
OPENAI_MAX_TOKENS=16000

# ============================================================================
# ANTHROPIC Configuration (Claude)
# ============================================================================
ANTHROPIC_API_KEY=your-anthropic-api-key-here
# Models: claude-haiku-4-5-20251001 (8K), claude-sonnet-4-5-20250929 (16K), claude-opus-4-5-20251101 (32K)
ANTHROPIC_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_MAX_TOKENS=16000

# ============================================================================
# GEMINI Configuration (Google AI)
# ============================================================================
# Use either GOOGLE_GENAI_API_KEY or GEMINI_API_KEY
GOOGLE_GENAI_API_KEY=your-gemini-api-key-here
# GEMINI_API_KEY=your-gemini-api-key-here
# Models: gemini-2.5-pro, gemini-2.0-flash, gemini-1.5-pro
GEMINI_MODEL=gemini-2.5-pro
# Gemini 2.5 Pro supports up to 65536 output tokens
GEMINI_MAX_TOKENS=65536

# ============================================================================
# MAX_TOKENS usado por los agentes (usar segun provider seleccionado)
# ============================================================================
MAX_TOKENS=8192

# ============================================================================
# Paths
# ============================================================================
WORKING_DIR=working
OUTPUT_DIR=outputs
LOG_DIR=logs

# ============================================================================
# MCTS Hyperparameters
# ============================================================================
MCTS_KM=30
MCTS_ND=10
MCTS_NI=2
MCTS_W=-0.07
MCTS_CUCT=1.414

# ============================================================================
# Execution
# ============================================================================
MAX_EXECUTION_TIME=7200
DEBUG_MODE=false
